{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bussiness Problem\n",
    "\n",
    "**What is Fake News?**\n",
    "\n",
    "Fake news is the deliberate presentation of (typically) false or misleading claims as news, where the claims are misleading by design.\n",
    "\n",
    "**How News and digital medium evolved?**\n",
    "\n",
    "The news media evolved from newspapers, tabloids, and magazines to a digital form such as online news platforms, blogs, social media feeds, and many news mobile app. News outlets benefitted from the widespread use of social media/mobile  platforms by providing updated news in near real time to its subscribers. \n",
    "\n",
    "It became easier for consumers to acquire the latest news at their fingertips. So, These digital media platforms become very powerful due to their easy accessibility to the world and  ability to allow users to discuss and share ideas and debate over issues such as democracy, education, health, research and history. \n",
    "\n",
    "However, apart from advantage false/fake news article on digital platforms are getting very common and mainly used with a negative intent for their own benifit such as political and finacial benefit, creating biased opinions, manipulating mindsets, and spreading absurdity. \n",
    "\n",
    "**How big this Problem ?**\n",
    "\n",
    "With the rapid adoption of Internet, social media and digital platforms (such as Facebook, Twitter, news portals or any social media), anybody can spread untrue and biased information. It is virtually impossible to prevent Fake News from being created\n",
    "There has been a rapid increase in the spread of fake news in the last decade, it's not limited any one domain like politics but covering various other domains such as sports, health, history, entertainment and also science and research. If we take example of 2016 US presidential election, there were lots biased and fake news published to influence. Another example could be of COVID-19, we generally come accross many misleading/fake news everyday which can have serious consequences and may lead to create panic among people and spread pandemic more rapidly. \n",
    "\n",
    "**What is Solution?**\n",
    "\n",
    "Therefore, It is important and absolute necessary to identif yand differentiate Fake News from real news. One of the ways to determine by expert and fact check of every news, but this is time consuming and requires skills which can not shared. Second, we can automate the detection of Fake News by using the techniques of Machine learning and Artificial Intelligence.\n",
    "The Online news content has diverse unstructure format data(such as documents, videos, and audios), here we will conetrate on text format news. With the advancement of and Natural language processing It is possible now that we can identify the deceptive and fake nature of article or sentence.\n",
    "\n",
    "There are wide spread study and experiment is happing in this area to identify the Fake news for all medium(Video, audio and Text) news. In our study we used [Fake news](https://www.kaggle.com/c/fake-news/data) to classify unreliable news article as Fake news using Deep learning Technique Sequence to Sequence programming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data source and few other library if not already downloaded.\n",
    "\n",
    "!wget -P ./input/ https://www.kaggle.com/c/fake-news/data\n",
    "!wget -P ./resource/glove/ http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip ./resource/glove/glove.6B.zip -d ./resource/glove/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import all library\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(filename,**kwargs):\n",
    "    raw_data=pd.read_csv(filename,**kwargs)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Directory Path\n",
    "\n",
    "root_dir = str(Path().resolve())\n",
    "input_dir = root_dir+\"/input/\"\n",
    "if not os.path.isdir(input_dir):\n",
    "    os.mkdir(input_dir)\n",
    "output_dir = root_dir+\"/output/\"\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "model_dir = root_dir+\"/model/\"\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "image_dir = 'images/'\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "Datasets: [Fake news](https://www.kaggle.com/c/fake-news/data)\n",
    "We will read here understand their charchteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### READ DATASTS\n",
    "\n",
    "news_df= read_data(\"input/train.csv\")\n",
    "submit_test = read_data('input/test.csv')\n",
    "submit_label = read_data('input/submit.csv')\n",
    "submit_test['label'] = submit_label.label\n",
    "print(\" Shape of News data :: \", news_df.shape)\n",
    "print(\" News data columns\", news_df.columns)\n",
    "print(\" Test columns\", submit_test.columns)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note : There no label for test dataset., So we will use train set for training and validation after split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Word startistics: min.mean, max and interquartile range\n",
    "\n",
    "text_len = news_df.text.str.split().str.len()\n",
    "text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title startistics \n",
    "\n",
    "title_len = news_df.title.str.split().str.len()\n",
    "title_len.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistic of Train and Test datasets are following: \n",
    "1. Attribute **text** having more words count with average of 760 words and 75% are less than 1000 word.\n",
    "2. Attribute **title** are small sentence with average of 12 words and 75% are less than 15 only.\n",
    "\n",
    "From following columns ['id', 'title', 'author', 'text', 'label'] we will not include id and author.\n",
    "\n",
    "**Our experiment would be with both text and title together**\n",
    "\n",
    "####  Count plot for both labels are\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"label\", data=news_df);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach :  Sequence Problem Neural Network\n",
    "\n",
    "### Sequence Problem\n",
    "- Sequences is a predictive modelling problem, in which you have a certain sequence of entries, and the task is to predict the next sequence. The input and output sequence could vary one to many sequence. The difficulty of this problem lies in the fact that sequences can vary in length. \n",
    "- Example of sequence problem:\n",
    "    - Text classification, snetimenent analysis: Where text are sequence of word, which could be long sentence\n",
    "    - Timeseries prediction: Stock, financial prediction\n",
    "    - Music generation\n",
    "    - Question Answering, Language modeling, Neural machine translation\n",
    "    - etc.\n",
    "    \n",
    "\n",
    "- Types of Sequence Problem\n",
    "    1. One-to-One: Where there is one input and one output. Typical example of a one-to-one sequence problems is the case where you have an image and you want to predict a single label for the image.\n",
    "    \n",
    "    2. Many-to-One: In many-to-one sequence problems, we have a sequence of data as input and we have to predict a single output. Text classification is sequence problems where we have an input sequence of words and we want to predict a single output tag.\n",
    "    \n",
    "    3. One-to-Many: In one-to-many sequence problems, we have single input and a sequence of outputs. A typical example is an image and its corresponding description.\n",
    "    \n",
    "    4. Many-to-Many: Many-to-many sequence problems involve a sequence input and a sequence output. For instance, stock prices of 7 days as input and stock prices of next 7 days as outputs. \n",
    "\n",
    "**Note: Challenge is to handle large input and output sequence.**\n",
    "    \n",
    "### Simple RNN, LSTM and GRU\n",
    "\n",
    "### RNN :  Recurrent Neural Network\n",
    "\n",
    "Recurrent Neural Networks (RNN) are designed to work with sequential data. RNN uses the previous information in the sequence to produce the current output. To understand this better takes an example sentence.\n",
    "\n",
    "    “My class is the best class.”\n",
    "\n",
    "At the time(T0 ), the first step is to feed the word “My” into the network. the RNN produces an output.\n",
    "\n",
    "At the time(T1 ), then at the next step we feed the word “class” and the activation value from the previous step. Now the RNN has information of both words “My” and “class”.\n",
    "\n",
    "And this process goes until all words in the sentence are given input. You can see the animation below to visualize and understand.\n",
    "\n",
    "![<'RNN workflow'>](./../resource/notebook/RNN-workflow.gif \"RNN Workflow\")\n",
    "\n",
    "#### RNN Architecture : It takes input from the previous step and current input.\n",
    "\n",
    "![<'RNN workflow'>](./../resource/notebook/RNN-gate-update.gif \"RNN Architecture\")\n",
    "\n",
    "#### Note:\n",
    "2. RNN weights and bias for all the nodes in the layer are same.\n",
    "3. In case of more steps it suffers from vanishing gradient more than other neural network architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem With RNN\n",
    "### Vanishing gradient and Exploding Gradient \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network has three major steps. \n",
    "- First, it does a forward pass and makes a prediction. \n",
    "- Second, it compares the prediction to the ground truth using a loss function. The loss function outputs an error value which is an estimate of how poorly the network is performing. \n",
    "- Last, it uses that error value to do back propagation which calculates the gradients for each node in the network.\n",
    "\n",
    "![<'Gradient Loss'>](./../resource/notebook/gradient_loss.gif)\n",
    "\n",
    "The gradient is the value used to adjust the networks internal weights, allowing the network to learn. The bigger the gradient, the bigger the adjustments and vice versa. \n",
    "Here is where the problem lies. When doing back propagation, each node in a layer calculates it’s gradient with respect to the effects of the gradients, in the layer before it. So if the adjustments to the layers before it is small, then adjustments to the current layer will be even smaller.\n",
    "\n",
    "That causes gradients to exponentially shrink as it back propagates down. The earlier layers fail to do any learning as the internal weights are barely being adjusted due to extremely small gradients. And that’s the vanishing gradient problem.\n",
    "\n",
    "To train a recurrent neural network, you use an application of back-propagation called back-propagation through time. The gradient values will exponentially shrink as it propagates through each time step.\n",
    "\n",
    "![<'BPTT Architecture'>](./../resource/notebook/many-to-one.JPG)\n",
    "\n",
    "Again, the gradient is used to make adjustments in the neural networks weights thus allowing it to learn. Small gradients mean small adjustments. That causes the early layers not to learn.\n",
    "\n",
    "1. The **current** state is a function of the **previous** state and the current input: \n",
    "$h_t = \\sigma(W_{R}h_{t-1} + W_{F}x_t)$\n",
    "2. The gradient of the loss $E_t$ at time $t$ on $W_{R}$ is a function of the current hidden state and model predictions $\\hat{y}_t$ at time t: \n",
    "$\\frac{\\partial E_t}{\\partial W_{R}} = \\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{R}}$\n",
    "3. Substituting (1) into (2) results in a **sum over all previous time-steps**:\n",
    "$\\frac{\\partial E_t}{\\partial W_{R}} = \\sum\\limits_{k=0}^{t} \\underbrace{\\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_k}\\frac{\\partial h_k}{\\partial W_{R}}}_\\text{product of gradient terms}$\n",
    "\n",
    "The problem is that $\\frac{\\partial h_t}{\\partial h_k} = \\Pi_j \\frac{\\partial h_j}{\\partial h_{j-1}}$ for j from $k + 1$ to $t$.  Because of this **repeated multiplicative interaction**, as the sequence length $t$ gets longer, the gradients themselves can get diminishingly small (**vanish**) or grow too large and result in numeric overflow (**explode**).\n",
    "\n",
    "**Two main Issues with RNN:**\n",
    "1. Due to short-term memory it’s too difficult for RNN to learn to preserve information over many timesteps.\n",
    "2. Hidden state is constently being rewritten due to vanishing gradient.\n",
    "\n",
    "**So need to solve these two issue using Memory and Gated Machinism, which can handle long sequence and their gredeint. These two issue would resolved by LSTM and GRU using multiple gate/memory cells.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM: Long Short Time Memory\n",
    "\n",
    "An LSTM has a similar control flow as a recurrent neural network but the differences are the operations within the LSTM’s cells that it propogates long sequence information. It solve the problem of short-term memory and vanishing gradients.\n",
    "\n",
    "![<'LSTM Architecture'>](./../resource/notebook/LSTM-gate_update.gif)\n",
    "\n",
    "**Forget gate**\n",
    "It controls what is kept vs forgotten, from previous cell state. In laymen terms, it will decide how much information from the previous state should be kept and forget remaining.\n",
    "\n",
    "**Input gate**\n",
    "To update the cell state, It controls what new information will be added into cell state.\n",
    "\n",
    "**Output gate**\n",
    "It controls which parts of the cell are output to the hidden state. It will determine what the next hidden state will be.\n",
    "\n",
    "**Cell State**\n",
    "This is memory state, which will carry forward throught all the layers and maintain long sequences.\n",
    "\n",
    "**Hidden state** : The final hidden state is dependent on the updated cell state,previous hidden state and current input content.\n",
    "\n",
    "<b>Summary</b>\n",
    "\n",
    "- LSTMs have three types of gates: input gates, forget gates, and output gates that control the flow of information.\n",
    "- The hidden layer output of LSTM includes the hidden state and the memory cell. Only the hidden state is passed into the output layer. The memory cell is entirely internal.\n",
    "- Cell state captures both short-term and long-term dependencies in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU: Gated Recurrent Units\n",
    "The workflow of GRU is same as LSTM but the difference is in the operations inside the GRU unit. Let’s see the architecture of it.\n",
    "\n",
    "![<'RNN workflow'>](./../resource/notebook/GRU-gate.gif \"GRU Architecture\")\n",
    "\n",
    "GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n",
    "\n",
    "**Update gate** : \n",
    "The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add. \n",
    "\n",
    "**Reset gate** : \n",
    "The gate is used to decide whether the previous state is important or not and decide how much past information to forget.\n",
    "\n",
    "**Hidden state** : It is just simply the same as the hidden state(activation) of RNN, which acts like neural network menory. The final hidden state is dependent on the update gate. Remove some content from hidden state, and write some new content.\n",
    "\n",
    "- In GRU the final cell state is directly passing as the activation to the next cell.\n",
    "\n",
    "<b>Summary</b>\n",
    "- If sigmoid(update gate) close to 1, then we can copy information in that unit through many steps!\n",
    "    Sigmoid Controls how much of past state should matter now.\n",
    "- If reset close to 0, ignore previous hidden state (allows the model to drop information that is irrelevant in the future).\n",
    "- Reset gates help capture short-term dependencies in sequences.\n",
    "- Update gates help capture long-term dependencies in sequences.\n",
    "\n",
    "Images Refrences : [RNN,GRU,LSTM](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many to One Sequence problem**, There news article would be sequence of multiple words and output would be only one.\n",
    "Identify when an article might be fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Build Many to One Sequence Problem for Text\n",
    "1. Clean Datasets: remove unused rows and columns, imput null etc\n",
    "2. Preprocessing: NLTK processing, \n",
    "3. Data Preperation\n",
    "    - Vectorization, Sequence data preperation, Padding to have equal sequence length\n",
    "4. Sequence representation: \n",
    "    - Onehot ecoding, Word embedding etc\n",
    "5. Build Emedding Layer\n",
    "6. Build Neural Network model\n",
    "7. Compile and fit model\n",
    "8. Hyperparamter tunning\n",
    "9. Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Datasets: \n",
    "   - Drop unused row and columns\n",
    "   - Null value imputation\n",
    "   - Remove special characters\n",
    "   - Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants Used for cleaning the datasets\n",
    "\n",
    "column_names = ['id', 'title', 'author', 'text', 'label']\n",
    "remove_columns = ['id','author']\n",
    "categorical_features = []\n",
    "target_col = ['label']\n",
    "text_features = ['title', 'text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Datasets\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "# Removed unused clumns\n",
    "def remove_unused_columns(df,column_names=remove_columns):\n",
    "    df = df.drop(column_names,axis=1)\n",
    "    return df\n",
    "\n",
    "# Impute null values with None\n",
    "def null_processing(feature_df):\n",
    "    for col in text_features:\n",
    "        feature_df.loc[feature_df[col].isnull(), col] = \"None\"\n",
    "    return feature_df\n",
    "\n",
    "def clean_datasets(df):\n",
    "    # remove unused column\n",
    "    df = remove_unused_columns(df)\n",
    "    #impute null values\n",
    "    df = null_processing(df)\n",
    "    return df\n",
    "\n",
    "## Cleaning text from unused characters\n",
    "def clean_text(text):\n",
    "    text = str(text).replace(r'http[\\w:/\\.]+', ' ')  # removing urls\n",
    "    text = str(text).replace(r'[^\\.\\w\\s]', ' ')  # remove everything but characters and punctuation\n",
    "    text = str(text).replace('[^a-zA-Z]', ' ')\n",
    "    text = str(text).replace(r'\\s\\s+', ' ')\n",
    "    text = text.lower().strip()\n",
    "    #text = ' '.join(text)    \n",
    "    return text\n",
    "\n",
    "## Nltk Preprocessing include:\n",
    "# Stop words,\n",
    "# Stemming and\n",
    "# Lemmetization\n",
    "# For our project we use only Stop word removal\n",
    "def nltk_preprocesing(text):\n",
    "    text = clean_text(text)\n",
    "    wordlist = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    #text = ' '.join([word for word in wordlist if word not in stopwords_dict])\n",
    "    #text = [ps.stem(word) for word in wordlist if not word in stopwords_dict]\n",
    "    text = ' '.join([wnl.lemmatize(word) for word in wordlist if word not in stopwords_dict])\n",
    "    return  text\n",
    "\n",
    "\n",
    "df = clean_datasets(news_df)\n",
    "df_test = clean_datasets(submit_test)\n",
    "\n",
    "df[\"text\"] = df.text.apply(nltk_preprocesing)\n",
    "df_test[\"text\"] = df_test.text.apply(nltk_preprocesing)\n",
    "\n",
    "df[\"title\"] = df.title.apply(nltk_preprocesing)\n",
    "df_test[\"title\"] = df_test.title.apply(nltk_preprocesing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis\n",
    "- Univariate Analysis: Statistical Analysis of Text, Word cloud\n",
    "- Bivariate Analysis : Bigram and Trigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single word word cloud\n",
    "Most frequent word appear with Bold and bigger font.\n",
    "#### Word cloud for all word in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud( background_color='black', width=800, height=600)\n",
    "\n",
    "text_cloud = wordcloud.generate(' '.join(df['text']))\n",
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(text_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud for True label news only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news = ' '.join(df[df['label']==0]['text']) \n",
    "wc = wordcloud.generate(true_news)\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud for Fake label news only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = ' '.join(df[df['label']==1]['text'])\n",
    "wc= wordcloud.generate(fake_news)\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### N-Gram\n",
    "\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent Bigram(Two word combination) of True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bigrams = (pd.Series(nltk.ngrams(true_news.split(), 2)).value_counts())[:20]\n",
    "true_bigrams.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "plt.title('Top 20 Frequently Occuring True news Bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('Number of Occurances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent Bigram(Two word combination) of Fake label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_bigrams = (pd.Series(nltk.ngrams(fake_news.split(), 2)).value_counts())[:20]\n",
    "fake_bigrams.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "plt.title('Top 20 Frequently Occuring Fake news Bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('Number of Occurances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Most frequent Trigram(Three word combination) of True label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bigrams = (pd.Series(nltk.ngrams(true_news.split(), 3)).value_counts())[:20]\n",
    "true_bigrams.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "plt.title('Top 20 Frequently Occuring True news Trigrams')\n",
    "plt.ylabel('Trigrams')\n",
    "plt.xlabel('Number of Occurances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent Trigram(Three word combination) of Fake label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_bigrams = (pd.Series(nltk.ngrams(fake_news.split(), 3)).value_counts())[:20]\n",
    "fake_bigrams.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "plt.title('Top 20 Frequently Occuring Fake news Trigrams')\n",
    "plt.ylabel('Trigrams')\n",
    "plt.xlabel('Number of Occurances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Hence from Word cloud as well as Different N-gram word anaysis it is difficult to conclude anything. So we will include all words to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training dataset into train and test for model selection and validation\n",
    "   - Merge all Text Feature\n",
    "   - Split datasets in Train, Test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Text features together\n",
    "def merge_text_features(df, text_featuers = text_features):\n",
    "    df['news']=df[text_featuers].agg(' '.join, axis=1)\n",
    "    print(\"Merge news text statistics::\\n \",df.news.str.split().str.len().describe())\n",
    "    return df\n",
    "\n",
    "## Preperaing Datasets\n",
    "def preparing_datasets(df):\n",
    "    XY = merge_text_features(df)\n",
    "    #XY[\"news\"] = XY.news.apply(clean_text)    \n",
    "    print(\" Cleaning as remove special character is done..\")\n",
    "    print(XY.head())\n",
    "    #XY[\"news\"] = XY.news.apply(nltk_preprocesing)\n",
    "    X = XY['news']\n",
    "    y = XY.label\n",
    "    print(\"Text len statistic after Merge news and preprocessing::\\n \",X.str.split().str.len().describe())\n",
    "    if  y.dtype=='object':\n",
    "        y= process_labels(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## preprocessing datasets\n",
    "print(\" Training data preprocessing \")\n",
    "X,y = preparing_datasets(df)\n",
    "\n",
    "print(\" Test data preprocessing \")\n",
    "X_test,y_test = preparing_datasets(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There we can observe that mean number word for each news record is 471 and 75% quartile words length is 650 only. \n",
    "So based on this statistic, we can fix our word sequence using any suitable size for all news length as equal size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Data Transformation :\n",
    "   - Tokenization\n",
    "   - Text sequence generation\n",
    "   - Pad the text sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization\n",
    "Keras tokenizer to convert each text into a sequence of words, and then create the vocabulary using method on the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split datasets into train test sets to evalute mode with test size: 20%\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "X_train,y_train = X,y\n",
    "print(\"Train data counts::\",X_train.shape)\n",
    "print(\"Test data counts::\",X_test.shape)\n",
    "\n",
    "oov_token = \"<OOV>\" # it will be added to word_index and used to replace\n",
    "                    #out-of-vocabulary words during text_to_sequence calls\n",
    "vocab_size = 100000 #the maximum number of words to keep, based\n",
    "                    #on word frequency\n",
    "    \n",
    "#tokenizer = Tokenizer(oov_token=oov_token)\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "text = X_train#['text']\n",
    "tokenizer.fit_on_texts(text)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index \")\n",
    "print(len(word_index.keys()))\n",
    "\n",
    "max_text_length = 100\n",
    "#vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_seqence_data(df,tokenizer):\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    # Create Sequence\n",
    "    print(\" Create Sequence of tokens \")\n",
    "    text_sequences = tokenizer.texts_to_sequences(df)\n",
    "\n",
    "#     # Missing words in Glove vectors\n",
    "#     words_used = [tokenizer.index_word[i] for i in range(1, vocab_size)]\n",
    "#     missing_words = set(words_used) - set(word_vec.index.values)\n",
    "#     print(len(missing_words))\n",
    "#     missing_word_index = [tokenizer.word_index[word] for word in missing_words]\n",
    "    \n",
    "#     # Deleting above missing words\n",
    "#     text_sequences = [[word for word in sentence if word not in missing_word_index] for sentence in text_sequences]\n",
    "    print(\"Text to sequence of Id:: \", text_sequences[0:1])\n",
    "    return text_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "Padding used to make length of sequence equal for all input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_type = \"post\" # Type of padding pre or post of input sequence\n",
    "trunction_type=\"post\" # Type of truncation used to truncate input sequence if exceed from maximum sequence length\n",
    "\n",
    "def pad_sequence_data(text_sequences,max_text_length):\n",
    "    # Pad the Sequences, because the sequences are not of the same length,\n",
    "    # so let’s pad them to make them of similar length\n",
    "    text_padded = pad_sequences(text_sequences, maxlen=max_text_length, padding=padding_type,\n",
    "                                          truncating=trunction_type)\n",
    "    return text_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text_seq = prepare_seqence_data(X_train,tokenizer)\n",
    "test_text_seq = prepare_seqence_data(X_test,tokenizer)\n",
    "\n",
    "train_text_padded = pad_sequence_data(train_text_seq,max_text_length)\n",
    "test_text_padded = pad_sequence_data(test_text_seq,max_text_length)\n",
    "\n",
    "print(\"Padded Sequence :: \", test_text_padded[0:1])\n",
    "\n",
    "print(\" Tokenizer detail :: \", tokenizer.document_count)\n",
    "print('Vocabulary size:', len(tokenizer.word_counts))\n",
    "print('Shape of data padded:', train_text_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### Text Encoding : Vectorization\n",
    "- One Hot encoding\n",
    "- count vector, eg: TFIDF\n",
    "- Word embedding\n",
    "    - Word2Vec\n",
    "    - GLOVE\n",
    "    - FASTEXT\n",
    "    - etc\n",
    "    \n",
    "**In this section we will use GLOVE to convert text into numeric vector representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "- Word embdding take care of word reprenetation \n",
    "- Word embedding algorithms can figure out tons of relationships from the text data. They use the idea of context and learn by seeing what word occurs near other words. It can represent words with fixed number of dimension instead of One-hot or count reprsentation with huge number of vocablury size.\n",
    "- There are many popular word embedding algorithms available out there. Glove and Word2Vec are the most popular ones.\n",
    "- We will use here Glove embedding\n",
    "- Glove trained on Gigaword corpus (400,000 word vectors based on 6 billion tokens), available as 50d, 100d, 200d, and 300d vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COnstants for Word Embeddings\n",
    "##########################\n",
    "## Embedding Parametrs ###\n",
    "\n",
    "emb_dim = 100\n",
    "embedding_type = 'glove'\n",
    "glove_dir = root_dir+'/resource/glove/'\n",
    "glove_file = glove_dir+\"glove.6B.\"+str(emb_dim)+\"d.txt\"\n",
    "vocab_size = len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create GLove Word embedding #########\n",
    "#### Download glove if not exist ########\n",
    "\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "def read_glove_embedings():\n",
    "    word_vec = pd.read_table(glove_file, sep=r\"\\s\", header=None, engine='python',\n",
    "                             encoding='iso-8859-1', error_bad_lines=False)\n",
    "    word_vec.set_index(0, inplace=True)\n",
    "\n",
    "    print('Found %s word vectors.' % len(word_vec))\n",
    "    print('politics',word_vec.head())\n",
    "    return word_vec\n",
    "\n",
    "# Golve embedding use tokenizer for \n",
    "# word index, vocab size\n",
    "def glove_embedings(tokenizer):\n",
    "    embeddings_index = read_glove_embedings()\n",
    "    embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "    #embedding_weights = np.zeros((10000, 50))\n",
    "    index_n_word = [(i, tokenizer.index_word[i]) for i in range(1, len(embedding_matrix)) if\n",
    "                    tokenizer.index_word[i] in embeddings_index.index]\n",
    "    idx, word = zip(*index_n_word)\n",
    "    embedding_matrix[idx, :] = embeddings_index.loc[word, :].values\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def onehot_embedding(tokenizer):\n",
    "    onehot_vec =  [one_hot(words, (len(tokenizer.word_counts) +1)) for words in tokenizer.word_index.keys()]\n",
    "    embedded_docs = pad_sequences(onehot_vec, padding='pre', maxlen=max_text_length)\n",
    "    return embedded_docs\n",
    "\n",
    "def build_embeddings(tokenizer):\n",
    "    vocab_len = vocab_size\n",
    "    print(\" vocab_len \", vocab_size)\n",
    "    \n",
    "    if embedding_type=='glove':\n",
    "        embedding_matrix =  glove_embedings(tokenizer)\n",
    "        print(\" Encoded word sequence:: \",embedding_matrix[0:10])\n",
    "        embeddingLayer = Embedding(input_dim=vocab_len, output_dim=emb_dim, input_length=max_text_length,\n",
    "                                   weights=[embedding_matrix], trainable=False)\n",
    "    elif embedding_type=='fasttext':\n",
    "        embedding_matrix =  fasttext_embedings()\n",
    "        embeddingLayer = Embedding(input_dim=vocab_len, output_dim=emb_dim, input_length=max_text_length,\n",
    "                                   weights=[embedding_matrix], trainable=False)\n",
    "    else:\n",
    "        embedding_matrix = onehot_embedding(tokenizer)\n",
    "        embeddingLayer = Embedding(input_dim=vocab_len, output_dim=emb_dim, input_length=max_text_length,\n",
    "                                   trainable=False)\n",
    "\n",
    "    return embeddingLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_layer = build_embeddings(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Sequence Model\n",
    "### Approach:\n",
    "1. Recurrent Neural Network: Simple RNN\n",
    "2. Long Short Time Memory\n",
    "3. GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to build Sequence Network\n",
    "\n",
    "1. Input Layer\n",
    "    - In Text case input woud be sequence of worde index\n",
    "    - (Max_sequence_length,number_training_size) \n",
    "    ---------------------------------------------\n",
    "2. Embedding Layer\n",
    "    - This layer convert each word index into vector representation of size(50,100,200,300)\n",
    "    - This could be One-hot or pre-trained(glove or fastetxt) or trainiable word2Vec(example: using gensim)\n",
    "    - One-hot would represent each word to size of vocabulary size\n",
    "    - In this experiment we used pre-trained model: glove with 50 and 100 dimension vector.\n",
    "    - embedding layer input: Input\n",
    "    - Output: (training_size,Max_sequence_length,embedding_dimension ) \n",
    "    ---------------------------------------------\n",
    "3. Hidden layer: RNN/LSTM/GRU\n",
    "    - input: embedding output\n",
    "    - output: (training_size,no_of_nurons)    \n",
    "    -------------------------------------------\n",
    "4. Stacked Layer: Multiple layer of RNN/LSTM/GRU can be repeated\n",
    "    - Stack layer need to return the same sequence from previous layer\n",
    "    - return_sequence = True\n",
    "    - because RNN/LSTM/GRU need the same sequence of input  \n",
    "    -------------------------------------------------------------------\n",
    "5. Dense Layer:\n",
    "    - collect final context output from Sequence layer\n",
    "    - Activation function: Relu    \n",
    "    ---------------------------------------------------\n",
    "6. Dropout/Batchnormalization after hidden layer if rquire, which is regularization technique to avoid over-fitting/vanishing gradiant. \n",
    "    \n",
    "    ----------------------------------------------------------------------------------------\n",
    "    \n",
    "7. Multiple Dense layer if require\n",
    "    \n",
    "    -------------------------------------------------------------------------------\n",
    "    \n",
    "8. Output Layer: \n",
    "    - Sigmoid: Activation function for binary class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter to Tune\n",
    "- Vocabulary length\n",
    "- max sequence length\n",
    "- Embedding size: 50/100/200/300\n",
    "- Neural Network: RNN/GRU/LSTM\n",
    "- No of Sequence layer \n",
    "- No.of Dense Hidden Layers\n",
    "- Dropout\n",
    "- Hidden layer dimension\n",
    "- No. of Epochs\n",
    "- Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "###  PARAMS  ###\n",
    "#####################\n",
    "sequence_neuron_size = 100\n",
    "hidden_layer_1 = 32\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "classifier = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, BatchNormalization, GRU, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from os.path import exists\n",
    "\n",
    "#Building Sequential network with\n",
    "#   Embeding Layer\n",
    "#   LSTM\n",
    "#   Dense\n",
    "#   Output Layer\n",
    "def build_network_lstm(embedding_layer):\n",
    "    \n",
    "    print(\" Building Sequential network \")\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(sequence_neuron_size))#, return_sequences=True))\n",
    "    #model.add(LSTM(100))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_network_GRU(embedding_layer):\n",
    "\n",
    "    print(\" Building GRU network \")\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GRU(sequence_neuron_size))#, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(hidden_layer_1, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def build_network_RNN(embedding_layer):\n",
    "\n",
    "    print(\" Building RNN network \")\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SimpleRNN(sequence_neuron_size))#, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(hidden_layer_1, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def train_model(model,X_train,y_train,X_test, y_test):\n",
    "    \n",
    "    # Compile Model with loss function, \n",
    "    # optimizer and metricecs as minimum parameter\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    \n",
    "    # Train model with Train and test set data\n",
    "    # Number of epochs, batch size as minimum parameter\n",
    "    history = model.fit(X_train, y_train, epochs=epochs,batch_size = batch_size ,validation_split=0.2)#validation_data=(X_test, y_test))   \n",
    "    return model,history\n",
    "\n",
    "def performance_history(history,model_type,name):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "#     plt.savefig(model_dir + image_dir+ model_type+'/' + name + \"_performance.jpeg\") \n",
    "\n",
    "def model_evaluation(model,X_test,y_test):\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "    return score\n",
    "\n",
    "def store_model(model,model_type,name):\n",
    "    # Store the model as json and \n",
    "    # store model weights as HDF5\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_dir+model_type+'/'+name+\"_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_dir +model_type+'/'+ name + \"_model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def performance_report(model,testX,testy):\n",
    "\n",
    "    time = date.today()\n",
    "\n",
    "    yhat_probs = model.predict(testX, verbose=0)\n",
    "    # predict crisp classes for test set\n",
    "    yhat_classes = model.predict(testX)\n",
    "    print(yhat_classes)\n",
    "    yhat_classes = np.argmax(yhat_classes, axis=-1)\n",
    "    print(yhat_classes)\n",
    "    # reduce to 1d array\n",
    "    yhat_probs = yhat_probs[:, 0]\n",
    "#     yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(testy, yhat_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(testy, yhat_classes)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(testy, yhat_classes)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(testy, yhat_classes)\n",
    "    print('F1 score: %f' % f1)\n",
    "    \n",
    "    if not\n",
    "    if exists(output_dir + 'report.csv'):\n",
    "        total_cost_df = pd.read_csv(output_dir + 'report.csv', index_col=0)\n",
    "    else:\n",
    "        total_cost_df = pd.DataFrame(\n",
    "                columns=['time', 'name', 'Precision', 'Recall', 'f1_score', 'accuracy'])\n",
    "\n",
    "    total_cost_df = total_cost_df.append(\n",
    "            {'time': time, 'name': name,'Precision': precision, 'Recall': recall, 'f1_score': f1,'accuracy':accuracy},\n",
    "            ignore_index=True)\n",
    "    total_cost_df.to_csv(output_dir + 'report.csv')\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Steps:\n",
    "- Build Network\n",
    "- Compile\n",
    "- Fit model to train\n",
    "- Hyperparamter Tunning, If required\n",
    "- Evaluate model with Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash = \"-\"\n",
    "name = \"Model_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlfoundry as mlf\n",
    "mlf.login()\n",
    "\n",
    "# from getpass import getpass\n",
    "# api_token = getpass(\"TrueFoundry API Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_api = mlf.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Build Network\n",
    "model_type='RNN'\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "name = \"Model_\" + str(epochs)+dash+str(batch_size)+dash+str(max_text_length)+dash+str(vocab_size)+dash\n",
    "\n",
    "model_rnn = build_network_RNN(embeding_layer)\n",
    "\n",
    "model_rnn,history = train_model(model_rnn,train_text_padded,y_train,test_text_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_history(history,model_type,name)\n",
    "# store_model(model_rnn,model_type,name)\n",
    "model_evaluation(model_rnn,test_text_padded,y_test)\n",
    "accuracy, precision, recall, f1=performance_report(model_rnn,test_text_padded,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_run = mlf_api.create_run(project_name=\"fake-news-sequence-classification\", run_name=\"RNN\")\n",
    "\n",
    "mlf_run.log_model(model_rnn, 'keras')\n",
    "metrics_dict = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1 score': f1\n",
    "}\n",
    "mlf_run.log_metrics(metrics_dict)\n",
    "\n",
    "y_train_probs = model_rnn.predict(train_text_padded, verbose=0)[:, 0]\n",
    "y_train_classes = (y_train_probs>0.5)\n",
    "\n",
    "train_df = pd.DataFrame(train_text_padded, columns=[str(i) for i in range(100)])\n",
    "train_df['targets'] = y_train\n",
    "train_df['predictions'] = y_train_classes\n",
    "\n",
    "\n",
    "y_test_probs = model_rnn.predict(test_text_padded, verbose=0)[:, 0]\n",
    "y_test_classes = (y_test_probs>0.5)\n",
    "\n",
    "test_df = pd.DataFrame(test_text_padded, columns=[str(i) for i in range(100)])\n",
    "test_df['targets'] = y_test\n",
    "test_df['predictions'] = y_test_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Build Network\n",
    "model_type = 'GRU'\n",
    "name = \"Model-\" + str(epochs)+dash+str(batch_size)+dash+str(max_text_length)+dash+str(vocab_size)+dash\n",
    "\n",
    "model_gru = build_network_GRU(embeding_layer)\n",
    "\n",
    "model_gru,history_gru = train_model(model_gru,train_text_padded,y_train,test_text_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'GRU'\n",
    "performance_history(history_gru,model_type,name)\n",
    "store_model(model_gru,model_type,name)\n",
    "model_evaluation(model_gru,test_text_padded,y_test)\n",
    "accuracy, precision, recall, f1=performance_report(model_gru,test_text_padded,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_run = mlf_api.create_run(project_name=\"fake-news-sequence-classification\", run_name=\"GRU\")\n",
    "\n",
    "mlf_run.log_model(model_gru, 'keras')\n",
    "metrics_dict = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1 score': f1\n",
    "}\n",
    "mlf_run.log_metrics(metrics_dict)\n",
    "\n",
    "y_train_probs = model_gru.predict(train_text_padded, verbose=0)[:, 0]\n",
    "y_train_classes = (y_train_probs>0.5)\n",
    "\n",
    "train_df = pd.DataFrame(train_text_padded, columns=[str(i) for i in range(100)])\n",
    "train_df['targets'] = y_train\n",
    "train_df['predictions'] = y_train_classes\n",
    "\n",
    "y_test_probs = model_gru.predict(test_text_padded, verbose=0)[:, 0]\n",
    "y_test_classes = (y_test_probs>0.5)\n",
    "\n",
    "test_df = pd.DataFrame(test_text_padded, columns=[str(i) for i in range(100)])\n",
    "test_df['targets'] = y_test\n",
    "test_df['predictions'] = y_test_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_type = 'LSTM'\n",
    "name = \"Model-\" + str(epochs)+dash+str(batch_size)+dash+str(max_text_length)+dash+str(vocab_size)+dash\n",
    "\n",
    "## Build Network\n",
    "model_lstm = build_network_lstm(embeding_layer,lstm_size)\n",
    "\n",
    "model_lstm,history_lstm = train_model(model_lstm,train_text_padded,y_train,test_text_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_history(history_lstm,model_type,name)\n",
    "store_model(model_lstm,model_type,name)\n",
    "model_evaluation(model_lstm,test_text_padded,y_test)\n",
    "accuracy, precision, recall, f1=performance_report(model_lstm,test_text_padded,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_run = mlf_api.create_run(project_name=\"fake-news-sequence-classification\", run_name=\"LSTM\")\n",
    "\n",
    "mlf_run.log_model(model_lstm, 'keras')\n",
    "metrics_dict = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1 score': f1\n",
    "}\n",
    "mlf_run.log_metrics(metrics_dict)\n",
    "\n",
    "y_train_probs = model_lstm.predict(train_text_padded, verbose=0)[:, 0]\n",
    "y_train_classes = (y_train_probs>0.5)\n",
    "\n",
    "train_df = pd.DataFrame(train_text_padded, columns=[str(i) for i in range(100)])\n",
    "train_df['targets'] = y_train\n",
    "train_df['predictions'] = y_train_classes\n",
    "\n",
    "y_test_probs = model_lstm.predict(test_text_padded, verbose=0)[:, 0]\n",
    "y_test_classes = (y_test_probs>0.5)\n",
    "\n",
    "test_df = pd.DataFrame(test_text_padded, columns=[str(i) for i in range(100)])\n",
    "test_df['targets'] = y_test\n",
    "test_df['predictions'] = y_test_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above experience, GRUs train faster and perform better than LSTMs on less training data.\n",
    "- GRUs are simpler and thus easier to modify, for example adding new gates in case of additional input to the network. It's just less code in general.\n",
    "- LSTMs should in theory remember longer sequences than GRUs and outperform them in tasks requiring modeling long-distance relations.\n",
    "\n",
    "*Reference Paper.*\n",
    "[Neural GPUs Learn Algorithms](https://arxiv.org/abs/1511.08228)\n",
    "[Comparative Study of CNN and RNN for Natural Language Processing](https://arxiv.org/abs/1702.01923)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs model sequential data, and are designed to capture how outputs at the current time step are influenced by the inputs that came before them. This is referred to as long-range dependencies. At a high level, this allows the model to remember what it has seen so far in order to better contextualize what it is seeing at the moment. \n",
    "\n",
    "It is what makes these models so powerful, but it is also what makes them so hard to train. \n",
    "RNNs Training use back-propagation through time (BPTT) to update the both weight of Previous layer as well as previous timestep instead of just Backpropogation through previous layer only.\n",
    "\n",
    "\n",
    "1. The **current** state is a function of the **previous** state and the current input: $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$\n",
    "2. The gradient of the loss $E_t$ at time $t$ on $W_{hh}$ is a function of the current hidden state and model predictions $\\hat{y}_t$ at time t: \n",
    "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{hh}}$\n",
    "3. Substituting (1) into (2) results in a **sum over all previous time-steps**:\n",
    "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\sum\\limits_{k=0}^{t} \\underbrace{\\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_k}\\frac{\\partial h_k}{\\partial W_{hh}}}_\\text{product of gradient terms}$\n",
    "\n",
    "The problem is that $\\frac{\\partial h_t}{\\partial h_k} = \\Pi_j \\frac{\\partial h_j}{\\partial h_{j-1}}$ for j from $k + 1$ to $t$.  Because of this **repeated multiplicative interaction**, as the sequence length $t$ gets longer, the gradients themselves can get diminishingly small (**vanish**) or grow too large and result in numeric overflow (**explode**). Ideally, in order to avoid these beheviors, the norms of the gradients should be close to 1. \n",
    "\n",
    "**Two main Issues with RNN:**\n",
    "1. Due to short-term memory it’s too difficult for RNN to learn to preserve information over many timesteps.\n",
    "2. Hidden state is constently being rewritten due to vanishing gradient.\n",
    "\n",
    "**So need to solve these two issue using Memory and Gated Machinism, which can handle long sequence and their gredeint. These two issue would resolved by LSTM and GRU using multiple gate/memory cells.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
